import torch.nn as nn
import torch
import numpy as np
import torch.optim as optim
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score, roc_auc_score
import torch.nn.functional as F
from tqdm import tqdm  # 导入 tqdm

def calculate_metrics(y_true, y_pred):
    y_true = torch.tensor(y_true)
    y_pred = torch.tensor(y_pred)
    y_true = y_true.to(torch.long)
    y_pred = y_pred.to(torch.long)

    confusion_matrix = torch.zeros((2, 2), dtype=torch.int64)
    for t, p in zip(y_true.view(-1), y_pred.view(-1)):
        confusion_matrix[t, p] += 1

    tp = confusion_matrix[1, 1]
    fp = confusion_matrix[0, 1]
    fn = confusion_matrix[1, 0]
    tn = confusion_matrix[0, 0]

    accuracy = (tp + tn) / (tp + fp + fn + tn)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return accuracy, f1, precision, recall


# Build CNN module and CBAM
class CNNNET(nn.Module):
    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings=None):
        super(CNNNET, self).__init__()
        if pretrained_embeddings is not None:
            # self.embedding = nn.Parameter(pretrained_embeddings, requires_grad=True)
            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)
        else:
            self.embedding = nn.Embedding(vocab_size, embedding_dim)

        self.BiLSTM = nn.LSTM(input_size=embedding_dim, hidden_size=2, num_layers=2, dropout=0.5, batch_first=True, bidirectional=True)
        # self.BiLSTM2 = nn.LSTM(input_size=32*2, hidden_size=32, num_layers=1, batch_first=True, bidirectional=True)
        self.Dropout = nn.Dropout(0.1)

        # 多层卷积 + 池化层
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten(1, 2)
        self.fc = nn.Linear(128*4, 32)
        self.fc1 = nn.Linear(32, 2)

    def forward(self, x1, x2):
        x1 = x1.to(device)
        x2 = x2.to(device)
        # 获取嵌入
        # x1_embedding = self.embedding[x1]
        # x2_embedding = self.embedding[x2]

        x1_embedding = self.embedding(x1)
        x2_embedding = self.embedding(x2)

        # 拼接 x1 和 x2 的嵌入向量
        x = x2_embedding - x1_embedding

        x, _ = self.BiLSTM(x)
        # x, _ = self.BiLSTM2(x)
        x = self.Dropout(x)

        # 卷积 + 池化层
        x = x.unsqueeze(1)  # 为 Conv1d 增加通道维度 (batch_size, 1, seq_length)
        x = self.conv1(x)
        x = F.relu(x)
        # x = self.pool(x)  # 池化层，减少维度
        x = self.conv2(x)
        x = F.relu(x)
        # x = self.pool(x)
        # 展平并通过全连接层
        # x = x.view(x.size(0), -1)  # Flatten the tensor
        x = self.flatten(x)
        x = self.fc(x)
        x = self.fc1(x)
        return x


# 自定义 Dataset 类来加载特征和标签
class TextPairDataset(Dataset):
    def __init__(self, x1_data, x2_data, labels):
        if isinstance(x1_data, pd.Series):
            x1_data = x1_data.to_numpy()
        if isinstance(x2_data, pd.Series):
            x2_data = x2_data.to_numpy()
        if isinstance(labels, pd.Series):
            labels = labels.to_numpy()

        self.x1_data = torch.tensor(x1_data, dtype=torch.long)
        self.x2_data = torch.tensor(x2_data, dtype=torch.long)
        self.labels = torch.tensor(labels, dtype=torch.long)

    def __len__(self):
        return len(self.x1_data)

    def __getitem__(self, idx):
        x1 = self.x1_data[idx]
        x2 = self.x2_data[idx]
        label = self.labels[idx]
        return x1, x2, label


def create_dataloader(x1_data, x2_data, labels, batch_size=256, shuffle=True):
    dataset = TextPairDataset(x1_data, x2_data, labels)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    return dataloader


def train_epoch(train_loader, model, criterion, optimizer):
    model.train()
    all_preds = []
    all_labels = []
    total_loss = 0.0

    for x1, x2, label in tqdm(train_loader, desc="Training"):
        optimizer.zero_grad()

        outputs = model(x1, x2)
        label = label.to(device)
        loss = criterion(outputs, label)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        _, predicted = torch.max(outputs, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(label.cpu().numpy())

    accuracy, f1, precision, recall = calculate_metrics(all_labels, all_preds)

    print(f"\nTrain Loss: {total_loss / len(train_loader):.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\n")


def evaluate_epoch(val_loader, model, criterion):
    model.eval()
    all_preds = []
    all_labels = []
    total_loss = 0.0

    with torch.no_grad():
        for x1, x2, label in tqdm(val_loader, desc="Evaluating"):
            outputs = model(x1, x2)
            label = label.to(device)
            loss = criterion(outputs, label)
            total_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(label.cpu().numpy())

    accuracy, f1, precision, recall = calculate_metrics(all_labels, all_preds)
    print(f"\nEvaluate Loss: {total_loss / len(val_loader):.4f} | Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1 Score: {f1:.4f}\n")

    return all_preds, all_labels


def save_predictions(x1_data, x2_data, labels, preds, filename="predictions.csv"):
    # Save x1, x2, original labels, and predictions to a CSV file
    results = pd.DataFrame({
        'X1': x1_data,
        'X2': x2_data,
        'Label': labels,
        'Prediction': preds
    })
    results.to_csv(filename, index=False)
    print(f"Results saved to {filename}")


# Split the data into training, validation, and test sets (80%:10%:10%)
data = pd.read_csv('data/go_2024/train_1_0_2.csv', sep='\t', header=None, names=["X1", "X2", "Label"])

# 80% training, 10% validation, 10% test split
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)



# Prepare train, validation, and test sets
x1_train, x2_train, y_train = train_data['X1'].values, train_data['X2'].values, train_data['Label'].values

x1_val, x2_val, y_val = val_data['X1'].values, val_data['X2'].values, val_data['Label'].values
x1_test, x2_test, y_test = test_data['X1'].values, test_data['X2'].values, test_data['Label'].values

# Create DataLoader
train_loader = create_dataloader(x1_train, x2_train, y_train, batch_size=256, shuffle=True)
val_loader = create_dataloader(x1_val, x2_val, y_val, batch_size=256, shuffle=False)
test_loader = create_dataloader(x1_test, x2_test, y_test, batch_size=256, shuffle=False)

# Initialize the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# embedding_matrix = np.load('data_init/CC_embedding_128.npy')
embedding_matrix = np.load('data_init/go_term_name_embeddings_weight.npy')
embedding_tensor = torch.from_numpy(embedding_matrix).float()

model = CNNNET(vocab_size=47903, embedding_dim=768, pretrained_embeddings=embedding_tensor).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train and evaluate the model
epochs = 10
for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}/{epochs}")
    train_epoch(train_loader, model, criterion, optimizer)
    evaluate_epoch(val_loader, model, criterion)

# Final evaluation on the test set
print("\nFinal evaluation on the test set")
preds, labels = evaluate_epoch(test_loader, model, criterion)

# Save x1, x2, original labels, and predicted results to CSV
save_predictions(x1_test, x2_test, y_test, preds, filename="test_predictions_1.csv")

# Compute additional metrics on the test set
all_preds, all_labels, probabilities = [], [], []
with torch.no_grad():
    for x1, x2, label in test_loader:
        outputs = model(x1.to(device), x2.to(device))
        _, predicted = torch.max(outputs, 1)
        all_probabilities = F.softmax(outputs, dim=1)[:, 1]
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(label.numpy())
        probabilities.extend(all_probabilities.cpu().numpy())

accuracy, f1, precision, recall = calculate_metrics(all_labels, all_preds)
auc_out = roc_auc_score(all_labels, probabilities)
aupr = average_precision_score(all_labels, probabilities)

print(f"Test Accuracy: {accuracy}")
print(f"Test F1 Score: {f1}")
print(f"Test Precision: {precision}")
print(f"Test Recall: {recall}")
print(f"Test AUC: {auc_out}")
print(f"Test AUPR: {aupr}")
